{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrEbQDoQkAPA",
        "outputId": "be591af1-d0da-4087-b7d0-8850747d2992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyswarms imbalanced-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHrAZfNhk5NX",
        "outputId": "109073ca-2c5f-439f-9d48-25446a097e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyswarms\n",
            "  Downloading pyswarms-1.3.0-py2.py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyswarms) (1.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyswarms) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from pyswarms) (3.10.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from pyswarms) (25.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pyswarms) (4.67.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyswarms) (1.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from pyswarms) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.17.0)\n",
            "Downloading pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyswarms\n",
            "Successfully installed pyswarms-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required library\n",
        "!pip install --upgrade gdown\n",
        "\n",
        "# Step 2: Download file using gdown (from full URL)\n",
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1t0mqjuEHOu_WZWh6wI_As1KvdZWHixHv\"\n",
        "output = \"dataset.csv\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Step 3: Load the CSV into pandas\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\", low_memory=False)\n",
        "print(\"✅ Loaded shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "thMuPVmlk7_c",
        "outputId": "6ce0eaf8-cc31-4b3a-9db2-b46a4c6edacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1t0mqjuEHOu_WZWh6wI_As1KvdZWHixHv\n",
            "From (redirected): https://drive.google.com/uc?id=1t0mqjuEHOu_WZWh6wI_As1KvdZWHixHv&confirm=t&uuid=22629571-9663-41b1-b24c-d6f0bf8d4648\n",
            "To: /content/dataset.csv\n",
            "100%|██████████| 435M/435M [00:05<00:00, 81.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded shape: (9446350, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  protocol flow_duration  src_bytes  dst_bytes dst_port total_fwd_pkts  \\\n",
              "0      udp      0.001055      132.0      164.0       53            NaN   \n",
              "1      udp      0.036133      528.0      304.0     1024            NaN   \n",
              "2      udp      0.001119      146.0      178.0       53            NaN   \n",
              "3      udp      0.001209      132.0      164.0       53            NaN   \n",
              "4      udp      0.001169      146.0      178.0       53            NaN   \n",
              "\n",
              "  total_bwd_pkts flow_bytes_s flow_pkts_s attack_type  \n",
              "0            NaN          NaN         NaN         NaN  \n",
              "1            NaN          NaN         NaN         NaN  \n",
              "2            NaN          NaN         NaN         NaN  \n",
              "3            NaN          NaN         NaN         NaN  \n",
              "4            NaN          NaN         NaN         NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f736a391-f875-408f-8bde-5640154cb9ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>protocol</th>\n",
              "      <th>flow_duration</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>dst_port</th>\n",
              "      <th>total_fwd_pkts</th>\n",
              "      <th>total_bwd_pkts</th>\n",
              "      <th>flow_bytes_s</th>\n",
              "      <th>flow_pkts_s</th>\n",
              "      <th>attack_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>udp</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>132.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>53</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>udp</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>528.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>1024</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>udp</td>\n",
              "      <td>0.001119</td>\n",
              "      <td>146.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>53</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>udp</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>132.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>53</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>udp</td>\n",
              "      <td>0.001169</td>\n",
              "      <td>146.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>53</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f736a391-f875-408f-8bde-5640154cb9ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f736a391-f875-408f-8bde-5640154cb9ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f736a391-f875-408f-8bde-5640154cb9ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-188afc39-23aa-4761-ac93-b84a8de60c7b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-188afc39-23aa-4761-ac93-b84a8de60c7b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-188afc39-23aa-4761-ac93-b84a8de60c7b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R62vqcrzlDSq",
        "outputId": "c178b3f2-6390-4760-8327-c10b962b87fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Alignment"
      ],
      "metadata": {
        "id": "zx3Z5HKoSwOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install gdown if needed\n",
        "!pip install --upgrade gdown\n",
        "\n",
        "# Step 2: Define file IDs and download them\n",
        "import gdown\n",
        "\n",
        "file_ids = {\n",
        "    \"CIC-IDS2017\": \"1va0WNVo05ue2v9AIwCe3NWBfZ_j2TzPX\",\n",
        "    \"CIC-IDS2018\": \"1URSMeZjRmw_7h4gpVqHdroMDv-kuDIUe\",\n",
        "    \"CIC-IDS2019\": \"1vhECd8KzkT6pA4gis82lvx71pt6K5Kfj\",  # updated ID\n",
        "    \"UNSW-NB15\":   \"1n45tmx5_GTTaqgLr8kv05HEOELXni62e\",\n",
        "    \"TON_IoT\":     \"14smq_StPP4WEDkPMfJO374fl1_wM-6-Z\"\n",
        "}\n",
        "\n",
        "# Step 3: Download and save with readable names\n",
        "for name, file_id in file_ids.items():\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", f\"{name}.csv\", quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5jYmuSqS88i",
        "outputId": "ab51e07b-51c3-4c46-e160-e24a525bd447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1va0WNVo05ue2v9AIwCe3NWBfZ_j2TzPX\n",
            "From (redirected): https://drive.google.com/uc?id=1va0WNVo05ue2v9AIwCe3NWBfZ_j2TzPX&confirm=t&uuid=5430dc4e-c30e-4a5b-b510-1c1a52e9b864\n",
            "To: /content/CIC-IDS2017.csv\n",
            "100%|██████████| 156M/156M [00:01<00:00, 90.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1URSMeZjRmw_7h4gpVqHdroMDv-kuDIUe\n",
            "To: /content/CIC-IDS2018.csv\n",
            "100%|██████████| 30.9M/30.9M [00:00<00:00, 125MB/s] \n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vhECd8KzkT6pA4gis82lvx71pt6K5Kfj\n",
            "From (redirected): https://drive.google.com/uc?id=1vhECd8KzkT6pA4gis82lvx71pt6K5Kfj&confirm=t&uuid=1c073b9b-e4eb-464f-8218-009e80cbd3e1\n",
            "To: /content/CIC-IDS2019.csv\n",
            "100%|██████████| 156M/156M [00:01<00:00, 99.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1n45tmx5_GTTaqgLr8kv05HEOELXni62e\n",
            "From (redirected): https://drive.google.com/uc?id=1n45tmx5_GTTaqgLr8kv05HEOELXni62e&confirm=t&uuid=de83008d-a223-4214-93d5-bf8a5af93be0\n",
            "To: /content/UNSW-NB15.csv\n",
            "100%|██████████| 604M/604M [00:05<00:00, 116MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14smq_StPP4WEDkPMfJO374fl1_wM-6-Z\n",
            "From (redirected): https://drive.google.com/uc?id=14smq_StPP4WEDkPMfJO374fl1_wM-6-Z&confirm=t&uuid=61932465-7048-4a92-a22a-734918f0e040\n",
            "To: /content/TON_IoT.csv\n",
            "100%|██████████| 226M/226M [00:03<00:00, 69.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gX1Ls0yd9r6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Colab working directory\n",
        "base = Path(\"/content\")\n",
        "\n",
        "sources = {\n",
        "    \"UNSW\":    base / \"unsw_nb15.csv\",\n",
        "    \"TON-IoT\": base / \"ton_iot.csv\",\n",
        "    \"CIC17\":   base / \"CIC-IDS2017.csv\",\n",
        "    \"CIC18\":   base / \"CIC-IDS2018.csv\",\n",
        "    \"CIC19\":   base / \"combined_cic19.csv\",\n",
        "}\n",
        "\n",
        "final_cols = [\n",
        "    \"protocol\", \"flow_duration\", \"src_bytes\", \"dst_bytes\", \"dst_port\",\n",
        "    \"total_fwd_pkts\", \"total_bwd_pkts\", \"flow_bytes_s\", \"flow_pkts_s\",\n",
        "    \"attack_type\"\n",
        "]\n",
        "\n",
        "def load_and_standardize(name, path):\n",
        "    df = pd.read_csv(path, encoding='ISO-8859-1', low_memory=False, on_bad_lines='skip')\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Rename per source\n",
        "    if name == \"UNSW\":\n",
        "        df = df.rename(columns={\n",
        "            'proto': 'protocol', 'dur': 'flow_duration',\n",
        "            'sbytes':'src_bytes','dbytes':'dst_bytes',\n",
        "            'dsport':'dst_port','attack_cat':'attack_type'\n",
        "        })\n",
        "    elif name == \"TON-IoT\":\n",
        "        df = df.rename(columns={\n",
        "            'proto':'protocol','duration':'flow_duration',\n",
        "            'src_bytes':'src_bytes','dst_bytes':'dst_bytes',\n",
        "            'dst_port':'dst_port','attack':'attack_type'\n",
        "        })\n",
        "    elif name == \"CIC17\":\n",
        "        df = df.rename(columns={\n",
        "            'Flow Duration':'flow_duration','Total Fwd Packets':'total_fwd_pkts',\n",
        "            'Total Backward Packets':'total_bwd_pkts','Flow Bytes/s':'flow_bytes_s',\n",
        "            'Flow Packets/s':'flow_pkts_s','Destination Port':'dst_port',\n",
        "            'TYPE':'attack_type'\n",
        "        })\n",
        "    elif name == \"CIC18\":\n",
        "        df = df.rename(columns={\n",
        "            'Flow Duration':'flow_duration','TotLen Fwd Pkts':'total_fwd_pkts',\n",
        "            'TotLen Bwd Pkts':'total_bwd_pkts','Flow Byts/s':'flow_bytes_s',\n",
        "            'Flow Pkts/s':'flow_pkts_s','Dst Port':'dst_port','Label':'attack_type'\n",
        "        })\n",
        "    elif name == \"CIC19\":\n",
        "        df = df.rename(columns={\n",
        "            'Flow Duration':'flow_duration','Total Fwd Packets':'total_fwd_pkts',\n",
        "            'Total Backward Packets':'total_bwd_pkts','Flow Bytes/s':'flow_bytes_s',\n",
        "            'Flow Packets/s':'flow_pkts_s','Label':'attack_type'\n",
        "        })\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown source: {name}\")\n",
        "\n",
        "    # Now select only columns that exist\n",
        "    existing = [c for c in final_cols if c in df.columns]\n",
        "    df = df[existing].copy()\n",
        "\n",
        "    # Add missing cols as NaN\n",
        "    for col in final_cols:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "\n",
        "    # Reorder\n",
        "    return df[final_cols]\n",
        "\n",
        "# Load & concatenate\n",
        "frames = []\n",
        "for name, path in sources.items():\n",
        "    if path.exists():\n",
        "        df_std = load_and_standardize(name, path)\n",
        "        print(f\"{name}: selected columns {df_std.columns.tolist()}\")\n",
        "        frames.append(df_std)\n",
        "    else:\n",
        "        print(f\"⚠️  {name} not found, skipping.\")\n",
        "\n",
        "combined = pd.concat(frames, ignore_index=True)\n",
        "print(\"✅ Combined shape:\", combined.shape)\n",
        "\n",
        "# Save\n",
        "combined.to_csv(base / \"final_combined_dataset.csv\", index=False)\n",
        "print(\"✅ Saved to final_combined_dataset.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCJ4NenGRwXS",
        "outputId": "2eb7a9f2-0a6f-4844-8e10-e102554366e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  UNSW not found, skipping.\n",
            "⚠️  TON-IoT not found, skipping.\n",
            "CIC17: selected columns ['protocol', 'flow_duration', 'src_bytes', 'dst_bytes', 'dst_port', 'total_fwd_pkts', 'total_bwd_pkts', 'flow_bytes_s', 'flow_pkts_s', 'attack_type']\n",
            "CIC18: selected columns ['protocol', 'flow_duration', 'src_bytes', 'dst_bytes', 'dst_port', 'total_fwd_pkts', 'total_bwd_pkts', 'flow_bytes_s', 'flow_pkts_s', 'attack_type']\n",
            "⚠️  CIC19 not found, skipping.\n",
            "✅ Combined shape: (1479946, 10)\n",
            "✅ Saved to final_combined_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "T0BZPzcs9tdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths where you downloaded the CSVs (update if needed)\n",
        "paths = {\n",
        "    \"CIC-IDS2017\": \"/content/CIC-IDS2017.csv\",\n",
        "    \"CIC-IDS2018\": \"/content/CIC-IDS2018.csv\",\n",
        "    \"CIC-IDS2019\": \"/content/CIC-IDS2019.csv\",\n",
        "    \"UNSW-NB15\":   \"/content/UNSW-NB15.csv\",\n",
        "    \"TON_IoT\":     \"/content/TON_IoT.csv\"\n",
        "}\n",
        "\n",
        "# Collect the set of columns for each dataset\n",
        "feature_counts = {}\n",
        "for name, path in paths.items():\n",
        "    df = pd.read_csv(path, low_memory=False, on_bad_lines='skip', encoding='ISO-8859-1')\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    feature_counts[name] = len(df.columns)\n",
        "\n",
        "# Build a summary DataFrame\n",
        "df_feat = pd.DataFrame.from_dict(feature_counts, orient='index', columns=['Original Feature Count'])\n",
        "df_feat['Common Retained'] = df_feat['Original Feature Count'].apply(lambda x: len(common_features))\n",
        "df_feat = df_feat.sort_index()\n",
        "print(df_feat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "ZlJI5520WdAP",
        "outputId": "d5bf6697-8432-4e9e-9843-4af7ac9702a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'common_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4591017da075>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Build a summary DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Original Feature Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Common Retained'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Original Feature Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdf_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4591017da075>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Build a summary DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Original Feature Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Common Retained'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Original Feature Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdf_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'common_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(df_feat.index, df_feat['Original Feature Count'], label='Original', alpha=0.6)\n",
        "plt.bar(df_feat.index, df_feat['Common Retained'],   label='Retained',  alpha=0.8)\n",
        "plt.ylabel(\"Number of Features\")\n",
        "plt.title(\"Feature Count Before vs. After Alignment\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rZJHLoWdWekf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the raw combined CSV (before any feature alignment)\n",
        "df_before = pd.read_csv(\"/content/dataset.csv\", low_memory=False)\n",
        "\n",
        "# 2. Clean column names\n",
        "df_before.columns = df_before.columns.str.strip().str.lower()\n",
        "\n",
        "# 3. Drop rows missing the raw label\n",
        "df_before = df_before.dropna(subset=['attack_type'])\n",
        "\n",
        "# 4. (Optional) Normalize label strings\n",
        "df_before['attack_type'] = (\n",
        "    df_before['attack_type']\n",
        "      .astype(str)\n",
        "      .str.strip()\n",
        "      .str.upper()\n",
        ")\n",
        "\n",
        "# 5. Plot the class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(\n",
        "    y='attack_type',\n",
        "    data=df_before,\n",
        "    order=df_before['attack_type'].value_counts().index\n",
        ")\n",
        "plt.title(\"Class Distribution Before Any Preprocessing\")\n",
        "plt.xlabel(\"Sample Count\")\n",
        "plt.ylabel(\"Attack Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RZgMEjJBYHej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the raw combined file\n",
        "raw_path = \"/content/dataset.csv\"\n",
        "df_raw = pd.read_csv(raw_path, low_memory=False)\n",
        "\n",
        "# 2. Clean column names\n",
        "df_raw.columns = df_raw.columns.str.strip().str.lower()\n",
        "\n",
        "# 3. Peek at attack_type values\n",
        "print(\"Sample attack_type values:\", df_raw['attack_type'].unique()[:10])\n",
        "print(\"Missing attack_type count:\", df_raw['attack_type'].isna().sum())\n",
        "print(\"Non-missing attack_type count:\", df_raw['attack_type'].notna().sum())\n",
        "\n",
        "# 4. Filter out true NaNs (and the string \"nan\" if present)\n",
        "df_plot = df_raw[\n",
        "    df_raw['attack_type'].notna() &\n",
        "    (df_raw['attack_type'].astype(str).str.lower() != 'nan')\n",
        "].copy()\n",
        "print(\"Rows with valid attack_type:\", len(df_plot))\n",
        "\n",
        "# 5. Normalize labels (trim whitespace, uppercase)\n",
        "df_plot['attack_type'] = df_plot['attack_type'].astype(str).str.strip().str.upper()\n",
        "\n",
        "# 6. Plot the top 20 attack types\n",
        "top_n = 20\n",
        "order = df_plot['attack_type'].value_counts().nlargest(top_n).index\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(\n",
        "    y='attack_type',\n",
        "    data=df_plot,\n",
        "    order=order,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title(f\"Top {top_n} Attack Types Before Preprocessing\")\n",
        "plt.xlabel(\"Sample Count\")\n",
        "plt.ylabel(\"Attack Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qfTLm_L-Y049"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the aligned combined dataset\n",
        "df = pd.read_csv(\"/content/dataset.csv\", low_memory=False)\n",
        "\n",
        "# 2. Missing‐value summary (before)\n",
        "num_rows = len(df)\n",
        "missing = df.isna().sum()\n",
        "missing_pct = (missing / num_rows * 100).round(2)\n",
        "missing_summary = pd.DataFrame({\n",
        "    \"Missing Count\": missing,\n",
        "    \"Missing %\":     missing_pct\n",
        "})\n",
        "print(missing_summary)\n",
        "\n",
        "# 3. Plot missing % before\n",
        "plt.figure(figsize=(8,4))\n",
        "missing_summary[\"Missing %\"].sort_values(ascending=False).plot(kind=\"bar\", color=\"skyblue\")\n",
        "plt.title(\"Missing % per Column (Before)\")\n",
        "plt.ylabel(\"% Missing\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Drop rows with >30% missing values (more than 3 of 10)\n",
        "df_clean = df[df.isna().sum(axis=1) <= 3].copy()\n",
        "print(f\"Rows before: {num_rows:,} → after drop: {len(df_clean):,}\")\n",
        "\n",
        "# 5. Impute numeric cols with median\n",
        "num_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "for col in num_cols:\n",
        "    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
        "\n",
        "# 6. Impute categorical cols with mode or placeholder\n",
        "for col in [\"protocol\", \"attack_type\"]:\n",
        "    if col in df_clean.columns:\n",
        "        if df_clean[col].notna().any():\n",
        "            mode_val = df_clean[col].mode().iloc[0]\n",
        "        else:\n",
        "            mode_val = \"UNKNOWN\"\n",
        "        df_clean[col] = df_clean[col].fillna(mode_val)\n",
        "\n",
        "# 7. Missing‐value summary (after)\n",
        "missing_after = df_clean.isna().sum()\n",
        "missing_after_pct = (missing_after / len(df_clean) * 100).round(2)\n",
        "missing_summary2 = pd.DataFrame({\n",
        "    \"Missing Count\": missing_after,\n",
        "    \"Missing %\":     missing_after_pct\n",
        "})\n",
        "print(missing_summary2)\n",
        "\n",
        "# 8. Plot missing % after\n",
        "plt.figure(figsize=(8,4))\n",
        "missing_summary2[\"Missing %\"].sort_values(ascending=False).plot(kind=\"bar\", color=\"coral\")\n",
        "plt.title(\"Missing % per Column (After Imputation)\")\n",
        "plt.ylabel(\"% Missing\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 9. Save cleaned data\n",
        "df_clean.to_csv(\"/content/cleaned_combined_dataset.csv\", index=False)\n",
        "print(\"✅ Cleaned data saved.\")\n"
      ],
      "metadata": {
        "id": "fKEyMk6qZE7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Load the cleaned dataset\n",
        "df = pd.read_csv(\"/content/dataset.csv\", low_memory=False)\n",
        "\n",
        "# 2. Drop any rows with missing or empty attack_type\n",
        "df = df[df['attack_type'].notna() & (df['attack_type'].astype(str).str.strip() != '')].copy()\n",
        "\n",
        "# 3. Normalize label strings\n",
        "df['attack_type'] = (\n",
        "    df['attack_type']\n",
        "      .astype(str)\n",
        "      .str.strip()\n",
        "      .str.upper()\n",
        ")\n",
        "\n",
        "# 4. Consolidate into 10 classes\n",
        "label_map = {\n",
        "    # Normal / Benign\n",
        "    'BENIGN':      'NORMAL',\n",
        "    'NORMAL':      'NORMAL',\n",
        "    '0.0':         'NORMAL',\n",
        "    '1.0':         'NORMAL',\n",
        "    # DoS variants\n",
        "    'DOS ATTACKS-HULK': 'DOS',\n",
        "    'DOS HULK':         'DOS',\n",
        "    'DOS':              'DOS',\n",
        "    'DDOS':             'DOS',\n",
        "    # Exploits\n",
        "    'EXPLOITS':   'EXPLOITS',\n",
        "    # Fuzzers\n",
        "    'FUZZERS':    'FUZZERS',\n",
        "    # Generic\n",
        "    'GENERIC':    'GENERIC',\n",
        "    # Reconnaissance\n",
        "    'RECONNAISSANCE': 'RECONNAISSANCE',\n",
        "    # Backdoor\n",
        "    'BACKDOOR':   'BACKDOOR',\n",
        "    # Shellcode\n",
        "    'SHELLCODE':  'SHELLCODE',\n",
        "    # Worms\n",
        "    'WORMS':      'WORMS'\n",
        "}\n",
        "df['attack_type'] = df['attack_type'].map(label_map).fillna('UNKNOWN')\n",
        "\n",
        "# 5. Encode the consolidated labels\n",
        "le = LabelEncoder()\n",
        "df['label_enc'] = le.fit_transform(df['attack_type'])\n",
        "mapping = {cls: idx for idx, cls in enumerate(le.classes_)}\n",
        "print(\"Encoded Label Mapping:\\n\", mapping)\n",
        "\n",
        "# 6. Plot the consolidated class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(\n",
        "    y='attack_type',\n",
        "    data=df,\n",
        "    order=df['attack_type'].value_counts().index,\n",
        "    palette='tab10'\n",
        ")\n",
        "plt.title(\"Class Distribution After Label Consolidation\")\n",
        "plt.xlabel(\"Sample Count\")\n",
        "plt.ylabel(\"Attack Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7. Save for next steps\n",
        "df.to_csv(\"/content/label_cleaned_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Zqn4Z0HFZPR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the cleaned dataset\n",
        "df = pd.read_csv(\"/content/dataset.csv\", low_memory=False)\n",
        "\n",
        "# 2. Drop columns with 100% missing (src_bytes, dst_bytes)\n",
        "for col in [\"src_bytes\", \"dst_bytes\"]:\n",
        "    if df[col].isna().all():\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "        print(f\"Dropped {col} (all missing)\")\n",
        "\n",
        "# 3. Encode 'protocol' categorical feature\n",
        "le_proto = LabelEncoder()\n",
        "df['protocol_enc'] = le_proto.fit_transform(df['protocol'].astype(str))\n",
        "\n",
        "# 4. (Re)encode 'attack_type' if not already numeric\n",
        "if 'label_enc' not in df.columns:\n",
        "    le_lbl = LabelEncoder()\n",
        "    df['label_enc'] = le_lbl.fit_transform(df['attack_type'].astype(str))\n",
        "\n",
        "# 5. Scale numeric features\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "# Exclude encoded labels from scaling\n",
        "numeric_cols = [c for c in numeric_cols if c not in ['protocol_enc', 'label_enc']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "print(\"Scaled numeric features:\", numeric_cols)\n",
        "\n",
        "# 6. Save final preprocessed dataset\n",
        "final_cols = numeric_cols + ['protocol_enc', 'label_enc']\n",
        "df[final_cols].to_csv(\"/content/final_preprocessed_dataset.csv\", index=False)\n",
        "print(\"✅ Final preprocessed dataset saved.\")\n",
        "\n",
        "# 7. (Optional) Plot one feature before vs. after scaling for demonstration\n",
        "#    Here we re-load unscaled for comparison\n",
        "df_raw = pd.read_csv(\"/content/cleaned_combined_dataset.csv\", low_memory=False)\n",
        "feat = numeric_cols[0]  # e.g., 'flow_duration'\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(df_raw[feat].dropna(), bins=50)\n",
        "plt.title(f\"{feat} Before Scaling\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(df[feat], bins=50)\n",
        "plt.title(f\"{feat} After Scaling\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KGqy_lm0ZOx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/label_cleaned_dataset.csv\", low_memory=False)\n",
        "print(\"Columns in label-cleaned file:\", df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "0zEWL4BomPVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Reload after inspection\n",
        "df = pd.read_csv(\"/content/label_cleaned_dataset.csv\", low_memory=False)\n",
        "\n",
        "# If raw 'protocol' exists, encode it; otherwise assume 'protocol_enc' is present\n",
        "if 'protocol' in df.columns:\n",
        "    le_proto = LabelEncoder()\n",
        "    df['protocol_enc'] = le_proto.fit_transform(df['protocol'].astype(str))\n",
        "else:\n",
        "    print(\"✅ No raw 'protocol' column—using existing 'protocol_enc'\")\n",
        "\n",
        "# Identify numeric columns to scale (exclude any encodings)\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "exclude = [c for c in ['protocol_enc', 'label_enc'] if c in numeric_cols]\n",
        "scale_cols = [c for c in numeric_cols if c not in exclude]\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
        "print(\"Scaled features:\", scale_cols)\n",
        "\n",
        "# Save final preprocessed dataset\n",
        "final_cols = scale_cols + ['protocol_enc', 'label_enc']\n",
        "df[final_cols].to_csv(\"/content/final_preprocessed_dataset.csv\", index=False)\n",
        "print(\"✅ Saved final preprocessed dataset with columns:\", final_cols)\n"
      ],
      "metadata": {
        "id": "ZbOmOtz2asiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the final preprocessed dataset\n",
        "df_final = pd.read_csv(\"/content/final_preprocessed_dataset.csv\", low_memory=False)\n",
        "\n",
        "# Identify feature columns (excluding label_enc)\n",
        "feature_cols = [col for col in df_final.columns if col != 'label_enc']\n",
        "\n",
        "# Determine grid size for plots\n",
        "n = len(feature_cols)\n",
        "cols = 3\n",
        "rows = (n + cols - 1) // cols\n",
        "\n",
        "# Plot histograms for each feature\n",
        "plt.figure(figsize=(cols * 4, rows * 3))\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    plt.subplot(rows, cols, i)\n",
        "    plt.hist(df_final[col].dropna(), bins=50)\n",
        "    plt.title(col)\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.suptitle(\"Feature Distributions After Scaling and Encoding\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P4KYdj9BmiVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from pyswarms.discrete.binary import BinaryPSO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the preprocessed dataset\n",
        "df = pd.read_csv(\"/content/final_preprocessed_dataset.csv\", low_memory=False)\n",
        "X = df.drop(columns=['label_enc']).values\n",
        "y = df['label_enc'].values\n",
        "feature_names = df.drop(columns=['label_enc']).columns.tolist()\n",
        "\n",
        "# 2. Define the PSO fitness function\n",
        "def fitness_function(particles):\n",
        "    \"\"\"\n",
        "    Each particle is a binary array of length n_features.\n",
        "    We select features where bit == 1, train a classifier,\n",
        "    and return 1 - mean accuracy (to minimize).\n",
        "    \"\"\"\n",
        "    n_particles = particles.shape[0]\n",
        "    scores = np.zeros(n_particles)\n",
        "    for i, particle in enumerate(particles):\n",
        "        if particle.sum() == 0:\n",
        "            # if no feature selected, penalize heavily\n",
        "            scores[i] = 1.0\n",
        "        else:\n",
        "            selected_idx = np.where(particle == 1)[0]\n",
        "            X_sel = X[:, selected_idx]\n",
        "            # 3‐fold CV for speed; increase for more robust selection\n",
        "            clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "            score = cross_val_score(clf, X_sel, y, cv=3, scoring='accuracy').mean()\n",
        "            scores[i] = 1.0 - score  # PSO minimizes the function\n",
        "    return scores\n",
        "\n",
        "# 3. Initialize PSO\n",
        "from pyswarms.discrete.binary import BinaryPSO\n",
        "\n",
        "# 3. Initialize PSO with required options\n",
        "n_features = X.shape[1]\n",
        "options = {\n",
        "    'c1': 1.5,    # cognitive coefficient\n",
        "    'c2': 1.5,    # social coefficient\n",
        "    'w':  0.7,    # inertia weight\n",
        "    'k':  30,     # number of neighbors for velocity update\n",
        "    'p':  2       # Minkowski p-norm (Euclidean distance)\n",
        "}\n",
        "\n",
        "pso = BinaryPSO(n_particles=30, dimensions=n_features, options=options)\n",
        "\n",
        "# 4. Run optimization\n",
        "best_cost, best_pos = pso.optimize(fitness_function, iters=40)\n",
        "\n",
        "# 5. Extract selected features\n",
        "selected_features = [name for bit, name in zip(best_pos, feature_names) if bit == 1]\n",
        "print(\"Selected Features (%d of %d):\" % (len(selected_features), n_features))\n",
        "print(selected_features)\n",
        "\n",
        "# 6. Plot feature‐selection mask\n",
        "plt.figure(figsize=(10,2))\n",
        "plt.bar(range(n_features), best_pos, color='teal')\n",
        "plt.yticks([0,1], ['Excluded','Included'])\n",
        "plt.xticks(range(n_features), feature_names, rotation=45, ha='right')\n",
        "plt.title(\"PSO Selected Feature Mask\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7. Save the reduced dataset\n",
        "df_reduced = df[selected_features + ['label_enc']]\n",
        "df_reduced.to_csv(\"/content/reduced_dataset_pso.csv\", index=False)\n",
        "print(\"✅ Reduced dataset saved to 'reduced_dataset_pso.csv'\")\n"
      ],
      "metadata": {
        "id": "tn9gx9WIa0eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fill attack_type NaNs with mode\n",
        "df['attack_type'] = df['attack_type'].fillna(df['attack_type'].mode()[0])\n",
        "\n",
        "# 2. Encode attack_type → label_enc BEFORE class filtering\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_target = LabelEncoder()\n",
        "df['label_enc'] = le_target.fit_transform(df['attack_type'])\n",
        "\n",
        "# 3. Now safe to remove rare classes\n",
        "class_counts = df['label_enc'].value_counts()\n",
        "valid_classes = class_counts[class_counts >= 2].index\n",
        "df = df[df['label_enc'].isin(valid_classes)]\n",
        "\n",
        "# 4. Define X and y\n",
        "X = df.drop(columns=['attack_type', 'label_enc'])\n",
        "y = df['label_enc']\n"
      ],
      "metadata": {
        "id": "ZgR5HGPzlGBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label distribution after cleaning:\")\n",
        "print(y.value_counts())\n"
      ],
      "metadata": {
        "id": "jd5aOQ_alIVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples = 100\n",
        "class_counts = df['label_enc'].value_counts()\n",
        "valid_classes = class_counts[class_counts >= min_samples].index\n",
        "df = df[df['label_enc'].isin(valid_classes)]\n",
        "\n",
        "# Redefine X and y\n",
        "X = df.drop(columns=['attack_type', 'label_enc'])\n",
        "y = df['label_enc']\n"
      ],
      "metadata": {
        "id": "oM6ksH9SlkpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Class Distribution Before Removing Rare Classes\")\n",
        "plt.ylabel(\"Sample Count\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TRBYyXx0loDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(y='attack_type', data=df, order=df['attack_type'].value_counts().index[:20])\n",
        "plt.title(\"Class Distribution Before Preprocessing\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Attack Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gY0CODu6LZ65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno\n",
        "msno.heatmap(df)\n",
        "plt.title(\"Missing Data Heatmap (Before Preprocessing)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B5HMJ044LZfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_corr = df.select_dtypes(include=['float64', 'int64']).corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df_corr, annot=False, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap Before Cleaning\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qTapgqoBLZDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)} samples\")\n",
        "print(f\"Val:   {len(X_val)} samples\")\n",
        "print(f\"Test:  {len(X_test)} samples\")\n"
      ],
      "metadata": {
        "id": "NM0HB68PlrkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When you first load the CSV\n",
        "df_full = pd.read_csv(\"/content/dataset.csv\", low_memory=False)\n",
        "\n",
        "# Optional: preview\n",
        "df_full.head()\n"
      ],
      "metadata": {
        "id": "aKen4U4-tfgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "figsize = (12, 1.2 * len(_df_11['dst_port'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(_df_11, x='dst_bytes', y='dst_port', inner='stick', palette='Dark2')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "y9L_yMGaB_bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "figsize = (12, 1.2 * len(_df_10['flow_duration'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(_df_10, x='dst_bytes', y='flow_duration', inner='stick', palette='Dark2')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lTPTyMaMB-a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "figsize = (12, 1.2 * len(_df_9['dst_port'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(_df_9, x='src_bytes', y='dst_port', inner='stick', palette='Dark2')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RfNq8VmNB9cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "plt.subplots(figsize=(8, 8))\n",
        "df_2dhist = pd.DataFrame({\n",
        "    x_label: grp['dst_port'].value_counts()\n",
        "    for x_label, grp in _df_7.groupby('flow_duration')\n",
        "})\n",
        "sns.heatmap(df_2dhist, cmap='viridis')\n",
        "plt.xlabel('flow_duration')\n",
        "_ = plt.ylabel('dst_port')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Y11fjprLANMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_6['dst_bytes'].plot(kind='line', figsize=(8, 4), title='dst_bytes')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LkBiKp-hAMZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_5['src_bytes'].plot(kind='line', figsize=(8, 4), title='src_bytes')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q5dXROvwALf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_4.plot(kind='scatter', x='src_bytes', y='dst_bytes', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-ib5wxq_AKhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "_df_3.groupby('dst_port').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lTM4TZbUAJm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "_df_2.groupby('flow_duration').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QiyeTCREAInS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_1['dst_bytes'].plot(kind='hist', bins=20, title='dst_bytes')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YRXdSYcVAHBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_0['src_bytes'].plot(kind='hist', bins=20, title='src_bytes')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9AtMYMjVAFue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJ_qP9JKAFE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When you first load the CSV\n",
        "df_full = pd.read_csv(\"/content/dataset.csv\", low_memory=False)\n",
        "\n",
        "# Optional: preview\n",
        "df_full.head()\n",
        "\n",
        "# Fill missing attack labels with 'Unknown' for before-plot\n",
        "df_full['attack_type'] = df_full['attack_type'].fillna('Unknown')\n",
        "before_counts = df_full['attack_type'].value_counts()\n",
        "\n",
        "# Reuse cleaned DataFrame\n",
        "df_clean = df.copy()\n",
        "df_clean['attack_type'] = le_target.inverse_transform(df_clean['label_enc'])\n",
        "after_counts = df_clean['attack_type'].value_counts()"
      ],
      "metadata": {
        "id": "6sD9sKaytr7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 18), sharey=True)\n",
        "\n",
        "# Before Cleaning\n",
        "sns.barplot(y=before_counts.index, x=before_counts.values, ax=axes[0])\n",
        "axes[0].set_title(\"Before Cleaning\", fontsize=16)\n",
        "axes[0].set_xlabel(\"Count\")\n",
        "axes[0].set_ylabel(\"Attack Type\")\n",
        "axes[0].tick_params(axis='y', labelsize=9)\n",
        "\n",
        "# After Cleaning\n",
        "sns.barplot(y=after_counts.index, x=after_counts.values, ax=axes[1])\n",
        "axes[1].set_title(\"After Cleaning (<100 Removed)\", fontsize=16)\n",
        "axes[1].set_xlabel(\"Count\")\n",
        "axes[1].set_ylabel(\"\")\n",
        "axes[1].tick_params(axis='y', labelsize=9)\n",
        "\n",
        "plt.suptitle(\"Attack Type Distribution — Before vs After Cleaning\", fontsize=20)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig(\"attack_type_distribution_comparison.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RvjgsHy2qfkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "# Convert 'protocol' to numerical using Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_clean['protocol_enc'] = le.fit_transform(df_clean['protocol'])\n",
        "\n",
        "# Select only numerical features for correlation\n",
        "numerical_features = df_clean.select_dtypes(include=np.number).columns\n",
        "df_clean_numeric = df_clean[numerical_features]\n",
        "\n",
        "# Drop original 'protocol' and use the encoded one for correlation\n",
        "sns.heatmap(df_clean_numeric.drop(columns=['label_enc', 'protocol_enc']).corr(), annot=False, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LwSHffzpmoVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Imbalance Ratio Plot\n"
      ],
      "metadata": {
        "id": "7hgwGYAXmgOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attack_type_map = {\n",
        "    \"DoS Hulk\": \"DoS\",\n",
        "    \"DoS attacks-Hulk\": \"DoS\",\n",
        "    \"DoS attacks-GoldenEye\": \"DoS\",\n",
        "    \"DoS GoldenEye\": \"DoS\",\n",
        "    \"DoS Slowloris\": \"DoS\",\n",
        "    \"DoS attacks-Slowloris\": \"DoS\",\n",
        "    \"DoS attacks-SlowHTTPTest\": \"DoS\",\n",
        "    \"DoS SlowHTTPTest\": \"DoS\",\n",
        "\n",
        "    \"Web Attack − Brute Force\": \"Brute Force\",\n",
        "    \"Brute Force\": \"Brute Force\",\n",
        "    \"Web Attack − XSS\": \"XSS\",\n",
        "    \"XSS\": \"XSS\",\n",
        "\n",
        "    \"Web Attack − SQL Injection\": \"SQL Injection\",\n",
        "    \"SQL Injection\": \"SQL Injection\",\n",
        "\n",
        "    \"DDoS\": \"DDoS\",\n",
        "    \"DDoS LOIC\": \"DDoS\",\n",
        "    \"DDoS HOIC\": \"DDoS\",\n",
        "\n",
        "    # Extend as needed...\n",
        "}\n"
      ],
      "metadata": {
        "id": "yDpbM2BpwroS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full['attack_type'] = df_full['attack_type'].replace(attack_type_map)\n"
      ],
      "metadata": {
        "id": "qiKJZIvOwtjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize all attack_type labels\n",
        "df_full['attack_type'] = df_full['attack_type'].str.strip().str.title()\n",
        "df_clean['attack_type'] = df_clean['attack_type'].str.strip().str.title()\n"
      ],
      "metadata": {
        "id": "A0LKPCMtw7TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize and then apply your label mapping\n",
        "df_full['attack_type'] = df_full['attack_type'].str.strip().str.title()\n",
        "df_clean['attack_type'] = df_clean['attack_type'].str.strip().str.title()\n",
        "\n",
        "attack_type_map = {\n",
        "    \"Dos Hulk\": \"Dos\",\n",
        "    \"Dos Attacks-Hulk\": \"Dos\",\n",
        "    \"Dos Attacks-Goldeneye\": \"Dos\",\n",
        "    \"Dos Goldeneye\": \"Dos\",\n",
        "    \"Dos Slowloris\": \"Dos\",\n",
        "    \"Dos Attacks-Slowloris\": \"Dos\",\n",
        "    \"Dos Attacks-Slowhttptest\": \"Dos\",\n",
        "    \"Dos Slowhttptest\": \"Dos\",\n",
        "\n",
        "    \"Web Attack − Brute Force\": \"Brute Force\",\n",
        "    \"Web Attack − Xss\": \"Xss\",\n",
        "    \"Web Attack − Sql Injection\": \"Sql Injection\",\n",
        "    # Add more if needed\n",
        "}\n",
        "\n",
        "df_full['attack_type'] = df_full['attack_type'].replace(attack_type_map)\n",
        "df_clean['attack_type'] = df_clean['attack_type'].replace(attack_type_map)\n"
      ],
      "metadata": {
        "id": "ATcGAD7gw-Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_ratios = df_clean['attack_type'].value_counts(normalize=True)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x=class_ratios.index, y=class_ratios.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Class Imbalance Ratios After Cleaning (Merged & Normalized Labels)\")\n",
        "plt.xlabel(\"attack_type\")\n",
        "plt.ylabel(\"Ratio\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"class_ratios_final_cleaned.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y2kfDyPdw_-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 🧹 Normalize attack_type in both DataFrames\n",
        "df_full['attack_type'] = df_full['attack_type'].str.strip().str.title()\n",
        "df_clean['attack_type'] = df_clean['attack_type'].str.strip().str.title()\n",
        "\n",
        "# 🔁 Recompute counts after cleaning\n",
        "before_counts = df_full['attack_type'].value_counts()\n",
        "after_counts = df_clean['attack_type'].value_counts()\n",
        "\n",
        "# 📊 Plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 18), sharey=True)\n",
        "\n",
        "# BEFORE Cleaning\n",
        "sns.barplot(y=before_counts.index, x=before_counts.values, ax=axes[0])\n",
        "axes[0].set_title(\"Before Cleaning\", fontsize=16)\n",
        "axes[0].set_xlabel(\"Count\")\n",
        "axes[0].set_ylabel(\"Attack Type\")\n",
        "axes[0].tick_params(axis='y', labelsize=9)\n",
        "\n",
        "# AFTER Cleaning\n",
        "sns.barplot(y=after_counts.index, x=after_counts.values, ax=axes[1])\n",
        "axes[1].set_title(\"After Cleaning (<100 Removed)\", fontsize=16)\n",
        "axes[1].set_xlabel(\"Count\")\n",
        "axes[1].set_ylabel(\"\")\n",
        "axes[1].tick_params(axis='y', labelsize=9)\n",
        "\n",
        "# Overall layout\n",
        "plt.suptitle(\"Attack Type Distribution — Before vs After Cleaning\", fontsize=20)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig(\"attack_type_distribution_comparison_cleaned.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SHwT0UXJxTAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import necessary packages after reset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Define simulated feature alignment and consolidation pipeline\n",
        "common_features = [\n",
        "    'flow_duration', 'total_fwd_pkts', 'total_bwd_pkts',\n",
        "    'flow_bytes_s', 'flow_pkts_s', 'fwd_pkt_len_mean', 'bwd_pkt_len_mean'\n",
        "]\n",
        "\n",
        "# Simulate loading 5 datasets (replace these paths with actual file paths as needed)\n",
        "paths = ['unsw_nb15.csv', 'ton_iot.csv', 'cic17.csv', 'cic18.csv', 'cic19.csv']\n",
        "dfs = []\n",
        "\n",
        "# Simulate placeholder DataFrame with appropriate structure if not real files exist\n",
        "for path in paths:\n",
        "    df_simulated = pd.DataFrame({\n",
        "        'flow_duration': np.random.rand(100),\n",
        "        'total_fwd_pkts': np.random.rand(100),\n",
        "        'total_bwd_pkts': np.random.rand(100),\n",
        "        'flow_bytes_s': np.random.rand(100),\n",
        "        'flow_pkts_s': np.random.rand(100),\n",
        "        'fwd_pkt_len_mean': np.random.rand(100),\n",
        "        'bwd_pkt_len_mean': np.random.rand(100),\n",
        "        'attack_cat': np.random.choice(['BENIGN', 'DoS', 'Backdoor', 'Fuzzers'], 100),\n",
        "        'label': np.random.choice([0, 1], 100)\n",
        "    })\n",
        "    dfs.append(df_simulated)\n",
        "\n",
        "# Step 2: Concatenate datasets\n",
        "combined = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Step 3: Missing value handling (simulate NaNs)\n",
        "combined.loc[combined.sample(frac=0.1).index, 'flow_duration'] = np.nan\n",
        "threshold = int(0.3 * len(common_features))\n",
        "combined = combined[combined[common_features].isnull().sum(axis=1) <= threshold]\n",
        "for col in combined.columns:\n",
        "    if combined[col].dtype == 'object':\n",
        "        combined[col].fillna(combined[col].mode()[0], inplace=True)\n",
        "    else:\n",
        "        combined[col].fillna(combined[col].median(), inplace=True)\n",
        "\n",
        "# Step 4: Label consolidation\n",
        "attack_map = {\n",
        "    'ANALYSIS': 'Analysis', 'BACKDOOR': 'Backdoor', 'DOS': 'DoS', 'DDOS': 'DoS',\n",
        "    'EXPLOITS': 'Exploits', 'FUZZERS': 'Fuzzers', 'GENERIC': 'Generic',\n",
        "    'BENIGN': 'Normal', 'NORMAL': 'Normal', 'RECONNAISSANCE': 'Reconnaissance',\n",
        "    'SHELLCODE': 'Shellcode', 'WORMS': 'Worms'\n",
        "}\n",
        "combined['attack_cat'] = combined['attack_cat'].str.upper().replace(attack_map)\n",
        "le = LabelEncoder()\n",
        "combined['attack_cat_enc'] = le.fit_transform(combined['attack_cat'])\n",
        "\n",
        "# Step 6: Scaling\n",
        "scaler = StandardScaler()\n",
        "X = combined[common_features]\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 7: SMOTE\n",
        "y = combined['attack_cat_enc']\n",
        "sm = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_scaled, y)\n",
        "\n",
        "# Plot class distribution before and after SMOTE\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.countplot(x=y, order=np.unique(y))\n",
        "plt.title(\"Class Distribution Before SMOTE\")\n",
        "plt.xlabel(\"Encoded Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.countplot(x=y_resampled, order=np.unique(y_resampled))\n",
        "plt.title(\"Class Distribution After SMOTE\")\n",
        "plt.xlabel(\"Encoded Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QKGV6BWIO_Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = X.copy()\n",
        "if 'protocol' in X.columns:\n",
        "    le_proto = LabelEncoder()\n",
        "    X['protocol'] = le_proto.fit_transform(X['protocol'])\n"
      ],
      "metadata": {
        "id": "NwoLjSEkxibN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save encoded file as new csv\n",
        "\n",
        "# Assuming 'df_clean' is your cleaned DataFrame with 'label_enc' column\n",
        "df_clean.to_csv('encoded_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Pol3_XtPyPfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📋 Columns in encoded CSV:\")\n",
        "print(df_after.columns.tolist())\n"
      ],
      "metadata": {
        "id": "8DuiqeN7P7bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "df_before = pd.read_csv(\"/content/dataset.csv\")\n",
        "df_after = pd.read_csv(\"/content/encoded_dataset.csv\")\n",
        "\n",
        "# Standardize column names\n",
        "df_before.columns = df_before.columns.str.strip().str.lower()\n",
        "df_after.columns = df_after.columns.str.strip().str.lower()\n",
        "\n",
        "# Clean 'attack_type' column\n",
        "df_before['attack_type'] = df_before['attack_type'].astype(str).str.strip().str.upper()\n",
        "df_after['attack_type'] = df_after['attack_type'].astype(str).str.strip().str.upper()\n",
        "\n",
        "# Group counts\n",
        "before_counts = df_before['attack_type'].value_counts()\n",
        "after_counts = df_after.groupby('attack_type')['label_enc'].count()\n",
        "\n",
        "# Align on union of all class labels\n",
        "all_labels = sorted(set(before_counts.index).union(after_counts.index))\n",
        "before = before_counts.reindex(all_labels, fill_value=0)\n",
        "after = after_counts.reindex(all_labels, fill_value=0)\n",
        "\n",
        "# Combine into DataFrame\n",
        "df_plot = pd.DataFrame({\n",
        "    'Before Encoding': before,\n",
        "    'After Encoding': after\n",
        "})\n",
        "\n",
        "# Plot side-by-side horizontal bars\n",
        "df_plot.sort_values('Before Encoding', ascending=True).plot(\n",
        "    kind='barh', figsize=(12, 12),\n",
        "    color=['steelblue', 'coral'],\n",
        "    width=0.8\n",
        ")\n",
        "plt.title(\"📊 Class Distribution Before vs After Label Encoding (Clean View)\")\n",
        "plt.xlabel(\"Sample Count\")\n",
        "plt.ylabel(\"Attack Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VRJSBhFmPiKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1H4H_xVQLAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "if 'protocol' in X_train.columns:\n",
        "    le_proto = LabelEncoder()\n",
        "    X_train['protocol'] = le_proto.fit_transform(X_train['protocol'])\n",
        "    X_val['protocol'] = le_proto.transform(X_val['protocol'])\n",
        "    X_test['protocol'] = le_proto.transform(X_test['protocol'])\n"
      ],
      "metadata": {
        "id": "lIew8Ry8xvIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_numeric_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
        "print(\"Non-numeric columns:\", non_numeric_cols)\n"
      ],
      "metadata": {
        "id": "y9b8RRNGyot4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['flow_duration', 'dst_port', 'total_fwd_pkts', 'total_bwd_pkts', 'flow_bytes_s', 'flow_pkts_s']:\n",
        "    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
        "    X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
        "    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
        "\n"
      ],
      "metadata": {
        "id": "CPoD9M6oyi90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['flow_duration', 'dst_port', 'total_fwd_pkts', 'total_bwd_pkts', 'flow_bytes_s', 'flow_pkts_s']:\n",
        "    median_val = X_train[col].median()\n",
        "    X_train[col].fillna(median_val, inplace=True)\n",
        "    X_val[col].fillna(median_val, inplace=True)\n",
        "    X_test[col].fillna(median_val, inplace=True)\n"
      ],
      "metadata": {
        "id": "_D6kX3VryxGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Infinite values in X_train:\", np.isinf(X_train).sum().sum())\n",
        "print(\"Very large values (>1e10):\", (X_train > 1e10).sum().sum())\n"
      ],
      "metadata": {
        "id": "VkFie2Dny4nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "X_val = X_val.replace([np.inf, -np.inf], np.nan)\n",
        "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n"
      ],
      "metadata": {
        "id": "aqtg3U2zy6Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.fillna(X_train.median())\n",
        "X_val   = X_val.fillna(X_train.median())  # use train's median to avoid leakage\n",
        "X_test  = X_test.fillna(X_train.median())\n"
      ],
      "metadata": {
        "id": "1XZPIL_Vy752"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "YyrOQDjPyzKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert back to DataFrames before saving\n",
        "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_val_df   = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "X_test_df  = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Add encoded labels\n",
        "X_train_df['label'] = y_train.values\n",
        "X_val_df['label']   = y_val.values\n",
        "X_test_df['label']  = y_test.values\n"
      ],
      "metadata": {
        "id": "_0_iWldgzI5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df.to_csv(\"train_scaled.csv\", index=False)\n",
        "X_val_df.to_csv(\"val_scaled.csv\", index=False)\n",
        "X_test_df.to_csv(\"test_scaled.csv\", index=False)\n",
        "\n",
        "print(\"✅ Saved: train_scaled.csv, val_scaled.csv, test_scaled.csv\")\n"
      ],
      "metadata": {
        "id": "OoKifCIbzKVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: 70% train, 30% temp\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
        "\n",
        "# Second split: 15% val, 15% test from 30%\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n"
      ],
      "metadata": {
        "id": "PC4gTyXqz0Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(scaler, \"standard_scaler.pkl\")\n",
        "joblib.dump(le_target, \"label_encoder.pkl\")\n"
      ],
      "metadata": {
        "id": "WVmbC6Puz5q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"Val:\\n\", y_val.value_counts(normalize=True))\n",
        "print(\"Test:\\n\", y_test.value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "4lzQ-Z9e0a0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_numeric_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
        "print(\"Non-numeric columns:\", non_numeric_cols)\n"
      ],
      "metadata": {
        "id": "smSjQyVA0tJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of known numeric columns with invalid string values\n",
        "cols_to_fix = ['flow_duration', 'dst_port', 'total_fwd_pkts', 'total_bwd_pkts', 'flow_bytes_s', 'flow_pkts_s']\n",
        "\n",
        "# Convert to numeric (invalid values → NaN)\n",
        "for col in cols_to_fix:\n",
        "    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
        "    X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
        "    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
        "\n",
        "# Replace NaN with median (from train set only)\n",
        "for col in cols_to_fix:\n",
        "    median_val = X_train[col].median()\n",
        "    X_train[col].fillna(median_val, inplace=True)\n",
        "    X_val[col].fillna(median_val, inplace=True)\n",
        "    X_test[col].fillna(median_val, inplace=True)\n"
      ],
      "metadata": {
        "id": "M32Ob77H0x8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NaNs per column in X_train:\")\n",
        "print(X_train.isna().sum())\n"
      ],
      "metadata": {
        "id": "JgqXNp8z08Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['src_bytes', 'dst_bytes']:\n",
        "    median_val = X_train[col].median()\n",
        "    X_train[col].fillna(median_val, inplace=True)\n",
        "    X_val[col].fillna(median_val, inplace=True)\n",
        "    X_test[col].fillna(median_val, inplace=True)\n"
      ],
      "metadata": {
        "id": "GicHszap1BDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Remaining NaNs:\", X_train.isna().sum().sum())\n"
      ],
      "metadata": {
        "id": "WdvIKQ8g1CjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Infs in X_train:\", np.isinf(X_train).sum().sum())\n",
        "print(\"Values > 1e10 in X_train:\", (X_train > 1e10).sum().sum())\n"
      ],
      "metadata": {
        "id": "cpBJ9FC91JWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "X_val = X_val.replace([np.inf, -np.inf], np.nan)\n",
        "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n"
      ],
      "metadata": {
        "id": "yuqnJHLW1KJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.fillna(X_train.median())\n",
        "X_val = X_val.fillna(X_train.median())  # use train's median for consistency\n",
        "X_test = X_test.fillna(X_train.median())\n"
      ],
      "metadata": {
        "id": "7ZnrGeRl1Ql0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ NaNs remaining:\", X_train.isna().sum().sum())\n",
        "print(\"✅ Infs remaining:\", np.isinf(X_train).sum().sum())\n"
      ],
      "metadata": {
        "id": "TjnUL89h1Sha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "5oAncB2c0lpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_sm_scaled = scaler.fit_transform(X_train_sm)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "1-x9JWPQ3Q0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(scaler, \"standard_scaler.pkl\")\n"
      ],
      "metadata": {
        "id": "ULENXvQF3Ut0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save compressed CSVs instead\n",
        "X_train_df.to_csv(\"train_dl.csv.gz\", index=False, compression=\"gzip\")\n",
        "X_val_df.to_csv(\"val_dl.csv.gz\", index=False, compression=\"gzip\")\n",
        "X_test_df.to_csv(\"test_dl.csv.gz\", index=False, compression=\"gzip\")\n",
        "\n",
        "print(\"✅ Saved compressed CSVs: train_dl.csv.gz, val_dl.csv.gz, test_dl.csv.gz\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LcOIyCvb3Y7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Apply SMOTE with Timer & Parallel Jobs"
      ],
      "metadata": {
        "id": "llabzI0U397A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import time\n",
        "\n",
        "# Remove n_jobs parameter\n",
        "smote = SMOTE(random_state=42)\n",
        "start = time.time()\n",
        "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
        "end = time.time()\n",
        "print(f\"✅ SMOTE completed in {end - start:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "4XfVDtsA3-gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_sm_scaled = scaler.fit_transform(X_train_sm)\n",
        "X_val_scaled      = scaler.transform(X_val)\n",
        "X_test_scaled     = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "XmwWBovRM7im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(scaler, \"standard_scaler.pkl\")\n"
      ],
      "metadata": {
        "id": "lv0SrbOwM98t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_sm_scaled, columns=X_train.columns)\n",
        "X_val_df   = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "X_test_df  = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "X_train_df['label'] = y_train_sm\n",
        "X_val_df['label']   = y_val.values\n",
        "X_test_df['label']  = y_test.values\n"
      ],
      "metadata": {
        "id": "wn4CkuJRNC3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train_dl.csv.gz /content/drive/MyDrive/\n",
        "!cp val_dl.csv.gz /content/drive/MyDrive/\n",
        "!cp test_dl.csv.gz /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "wZJixUTGNEr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Class Distribution"
      ],
      "metadata": {
        "id": "0-xQHmA74Aoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"Unique labels after SMOTE:\", np.unique(y_train_sm))\n",
        "print(\"Total samples:\", len(y_train_sm))\n"
      ],
      "metadata": {
        "id": "5vdAMZMqNbgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df['attack_type'])\n"
      ],
      "metadata": {
        "id": "VDIcM-bKPJmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ Sample just 100,000 rows for plotting\n",
        "y_sample = pd.Series(y_train_sm).sample(100_000, random_state=42)\n",
        "\n",
        "# ✅ Map integer labels to attack names using your LabelEncoder\n",
        "label_to_attack = dict(enumerate(le.classes_))\n",
        "attack_names = y_sample.map(label_to_attack)\n",
        "\n",
        "# ✅ Wrap into DataFrame for plotting\n",
        "y_sample_df = pd.DataFrame({'attack_type': attack_names})\n",
        "\n",
        "# ✅ Plot\n",
        "plt.figure(figsize=(20, 8))\n",
        "sns.countplot(data=y_sample_df, x='attack_type', order=y_sample_df['attack_type'].value_counts().index, palette='tab20')\n",
        "plt.title(\"Sampled Class Distribution After SMOTE (100,000 rows)\", fontsize=16)\n",
        "plt.xlabel(\"Attack Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a8nkUyLi4DQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 📌 Mapping label indices back to original attack type names\n",
        "label_to_name = dict(enumerate(le.classes_))\n",
        "\n",
        "# 🔹 Map encoded labels to attack type names\n",
        "y_train_named = y_train.map(label_to_name)\n",
        "y_sm_named_sample = pd.Series(y_train_sm).sample(100_000, random_state=42).map(label_to_name)\n",
        "\n",
        "# 🔹 Create DataFrames for plotting\n",
        "y_train_df = pd.DataFrame({'Attack Type': y_train_named})\n",
        "y_sm_df = pd.DataFrame({'Attack Type': y_sm_named_sample})\n",
        "\n",
        "# 🔹 Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# 📊 Create side-by-side subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 10), sharey=True)\n",
        "\n",
        "# ▶️ Before SMOTE\n",
        "sns.countplot(data=y_train_df, y='Attack Type',\n",
        "              order=y_train_df['Attack Type'].value_counts().index,\n",
        "              ax=axes[0], palette=\"viridis\")\n",
        "axes[0].set_title(\"Before SMOTE\", fontsize=14)\n",
        "axes[0].set_xlabel(\"Sample Count\", fontsize=12)\n",
        "axes[0].set_ylabel(\"Attack Type\", fontsize=12)\n",
        "\n",
        "# ▶️ After SMOTE (sampled)\n",
        "sns.countplot(data=y_sm_df, y='Attack Type',\n",
        "              order=y_sm_df['Attack Type'].value_counts().index,\n",
        "              ax=axes[1], palette=\"viridis\")\n",
        "axes[1].set_title(\"After SMOTE (Sample of 100,000)\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Sample Count\", fontsize=12)\n",
        "axes[1].set_ylabel(\"\")  # Remove y-axis label to avoid repetition\n",
        "\n",
        "# 🔠 Overall plot title\n",
        "plt.suptitle(\"Comparison of Attack Type Distribution — Before and After SMOTE Oversampling\", fontsize=18)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "# 💾 Optional: Save as PNG for thesis figures\n",
        "plt.savefig(\"attack_distribution_before_after_smote.png\", dpi=300)\n",
        "\n",
        "# 📊 Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dTiySwaiN0Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MuzGzyg-8MGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Series or Flow Duration **Distribution**\n",
        "\n"
      ],
      "metadata": {
        "id": "CZlOPL264GAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['flow_duration'].sample(100_000, random_state=42), bins=100, kde=True)\n",
        "plt.title(\"Flow Duration Distribution (Sample of 100K)\")\n",
        "plt.xlabel(\"Flow Duration\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QIYcDPnhQpfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Feature Correlation Heatmap"
      ],
      "metadata": {
        "id": "oXs4pkStQssr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 12))\n",
        "corr = df.select_dtypes(include='number').corr()\n",
        "sns.heatmap(corr, cmap='coolwarm', annot=False, fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sLkZ_8h0Qu5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE or PCA 2D Visualization"
      ],
      "metadata": {
        "id": "cpooI9qrQxGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Scale\n",
        "# 1. Create a copy of X to avoid modifying the original DataFrame\n",
        "X_temp = X[:5000].copy()\n",
        "\n",
        "# 2. If 'protocol' is in the DataFrame, encode it to numerical\n",
        "if 'protocol' in X_temp.columns:\n",
        "    le = LabelEncoder()\n",
        "    X_temp['protocol'] = le.fit_transform(X_temp['protocol'])\n",
        "\n",
        "# 3. Convert hexadecimal strings to integers before scaling\n",
        "X_temp = X_temp.apply(pd.to_numeric, errors='coerce').fillna(0) # Convert hex to int and fill NaN with 0\n",
        "\n",
        "# 4. Now apply StandardScaler to the modified DataFrame\n",
        "X_scaled = StandardScaler().fit_transform(X_temp)\n",
        "y_sample = y[:5000]\n",
        "\n",
        "# Reduce\n",
        "X_embedded = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_scaled)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=pd.Series(y_sample).map(label_to_name), palette='tab10', s=50, alpha=0.8)\n",
        "plt.title(\"t-SNE Projection of Sampled Features\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JKd6wYZGQxvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Data Heatmap"
      ],
      "metadata": {
        "id": "l4layCBNQ2gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno\n",
        "msno.matrix(df.sample(10000))\n"
      ],
      "metadata": {
        "id": "kxSFmLz6Q2_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Value Distributions (Histogram Grid)"
      ],
      "metadata": {
        "id": "4tnPUh4ZQ4ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "important_features = ['flow_duration', 'src_bytes', 'dst_bytes', 'flow_bytes_s', 'total_fwd_pkts', 'total_bwd_pkts']\n",
        "\n",
        "df[important_features].hist(bins=50, figsize=(12, 8))\n",
        "plt.suptitle(\"Feature Distributions Before Scaling\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JBTT32cEQ6Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Imbalance Before SMOTE (Pie Chart)"
      ],
      "metadata": {
        "id": "kZ9BSLWwQ7ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " t-SNE / PCA Clustering Visualization"
      ],
      "metadata": {
        "id": "xVSAaoTSQ-8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd  # Import pandas if not already imported\n",
        "\n",
        "# 1. Create a copy of X to avoid modifying the original DataFrame\n",
        "X_temp = X[:5000].copy()\n",
        "\n",
        "# 2. If 'protocol' is in the DataFrame, encode it to numerical\n",
        "if 'protocol' in X_temp.columns:\n",
        "    le = LabelEncoder()\n",
        "    X_temp['protocol'] = le.fit_transform(X_temp['protocol'])\n",
        "\n",
        "# 3. Convert hexadecimal strings in 'dst_port' to integers\n",
        "X_temp['dst_port'] = X_temp['dst_port'].apply(lambda x: int(x, 16) if isinstance(x, str) and x.startswith('0x') else x)\n",
        "\n",
        "# 4. Convert all columns to numeric (invalid values → NaN)\n",
        "X_temp = X_temp.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# 5. Fill NaN with median (from this subset only)\n",
        "X_temp = X_temp.fillna(X_temp.median())\n",
        "\n",
        "# 6. Now apply StandardScaler to the modified DataFrame\n",
        "X_scaled = StandardScaler().fit_transform(X_temp)\n",
        "y_sample = y[:5000]\n",
        "\n",
        "# ***DIAGNOSE***: Print number of NaNs to check\n",
        "print(\"Number of NaNs in X_scaled:\", np.isnan(X_scaled).sum())\n",
        "\n",
        "# ***FIX***: Impute NaNs using SimpleImputer if any are found\n",
        "if np.isnan(X_scaled).sum() > 0:\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy='median')  # Or other strategies like 'mean', 'most_frequent'\n",
        "    X_scaled = imputer.fit_transform(X_scaled)\n",
        "\n",
        "\n",
        "# ... (rest of your t-SNE code)\n",
        "X_embedded = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=pd.Series(y[:5000]).map(label_to_name), palette='tab10', s=40)\n",
        "plt.title(\"t-SNE Visualization of Sampled Attack Types\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KcYwhwx6RJuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Heatmap (Features)\n"
      ],
      "metadata": {
        "id": "j5RPdGQVRNMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(pd.DataFrame(X_train_sm, columns=X.columns).corr(), cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Heatmap (Train Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ep2WJIrARQ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X3tqofgnYDOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    df[top_features].corr(),\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap='coolwarm',\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    annot_kws={\"size\": 10, \"color\": \"black\"}\n",
        ")\n",
        "\n",
        "plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"feature_correlation_heatmap_annotated.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U578Rj2DaUT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "important_features = ['flow_duration', 'src_bytes', 'dst_bytes', 'flow_bytes_s', 'total_fwd_pkts', 'total_bwd_pkts']\n",
        "df[important_features].sample(100_000).hist(bins=50, figsize=(12, 8), edgecolor='black')\n",
        "plt.suptitle(\"Distributions of Important Features (Sample of 100K)\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jtfvaMLcYBr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[top_features].isna().sum()\n"
      ],
      "metadata": {
        "id": "wXY1HEF7btpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: List your selected features\n",
        "top_features = [\n",
        "    'flow_duration', 'src_bytes', 'dst_bytes',\n",
        "    'total_fwd_pkts', 'total_bwd_pkts',\n",
        "    'flow_bytes_s', 'flow_pkts_s'\n",
        "]\n",
        "\n",
        "# Step 2: Force all values to numeric (convert strings to floats, NaNs stay)\n",
        "df_imputed = df[top_features].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Step 3: Median imputation\n",
        "df_imputed = df_imputed.fillna(df_imputed.median())\n",
        "\n",
        "# Step 4: Sample 50,000 for fast plotting\n",
        "df_sample = df_imputed.sample(50000, random_state=42)\n",
        "\n",
        "# Step 5: Plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "for i, feature in enumerate(top_features):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    sns.histplot(df_sample[feature], bins=50, kde=True, color='darkcyan')\n",
        "    plt.title(f\"{feature} Distribution\", fontsize=12)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.suptitle(\"📊 Feature Distributions After Type Fix + Median Imputation (Sampled 50K)\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig(\"feature_distribution_fixed.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xriqBRocYSD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ─── 1. Define top features ─────────────────────────────\n",
        "top_features = [\n",
        "    'flow_duration', 'src_bytes', 'dst_bytes',\n",
        "    'total_fwd_pkts', 'total_bwd_pkts',\n",
        "    'flow_bytes_s', 'flow_pkts_s'\n",
        "]\n",
        "\n",
        "# ─── 2. Convert to numeric ───────────────────────────────\n",
        "df_numeric = df[top_features].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# ─── 3. Replace inf and extreme values ──────────────────\n",
        "df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# ─── 4. Fill NaNs with median ───────────────────────────\n",
        "df_imputed = df_numeric.fillna(df_numeric.median())\n",
        "\n",
        "# ─── 5. Sample 100,000 for visualization ────────────────\n",
        "df_sample = df_imputed.sample(100_000, random_state=42)\n",
        "\n",
        "# ─── 6. Scale data ──────────────────────────────────────\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_sample), columns=top_features)\n",
        "\n",
        "# ─── 7. Plot Before & After Scaling ─────────────────────\n",
        "fig, axes = plt.subplots(len(top_features), 2, figsize=(14, 4 * len(top_features)))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    sns.histplot(df_sample[feature], bins=50, kde=True, ax=axes[i, 0], color='cornflowerblue')\n",
        "    axes[i, 0].set_title(f\"{feature} — Before Scaling\")\n",
        "    axes[i, 0].set_xlabel(feature)\n",
        "\n",
        "    sns.histplot(df_scaled[feature], bins=50, kde=True, ax=axes[i, 1], color='seagreen')\n",
        "    axes[i, 1].set_title(f\"{feature} — After Scaling (StandardScaler)\")\n",
        "    axes[i, 1].set_xlabel(feature)\n",
        "\n",
        "plt.suptitle(\"📊 Feature Distributions — Before vs After Standard Scaling\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "plt.savefig(\"feature_distributions_scaling_comparison.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LSiPMRavYcXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training: CNN + BiLSTM + Attention"
      ],
      "metadata": {
        "id": "_-Ii_tiYc_pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load training set\n",
        "train_df = pd.read_csv(\"/content/train_dl.csv.gz\")\n",
        "val_df = pd.read_csv(\"/content/val_dl.csv.gz\")\n",
        "test_df = pd.read_csv(\"/content/test_dl.csv.gz\")\n",
        "\n",
        "# Count top 10 frequent labels\n",
        "top_N = 10\n",
        "top_labels = train_df['label'].value_counts().nlargest(top_N).index.tolist()\n",
        "\n",
        "# Filter only top N classes\n",
        "train_df = train_df[train_df['label'].isin(top_labels)]\n",
        "val_df   = val_df[val_df['label'].isin(top_labels)]\n",
        "test_df  = test_df[test_df['label'].isin(top_labels)]\n",
        "\n",
        "# Re-map labels to 0..(N-1)\n",
        "label_map = {old: new for new, old in enumerate(sorted(top_labels))}\n",
        "train_df['label'] = train_df['label'].map(label_map)\n",
        "val_df['label']   = val_df['label'].map(label_map)\n",
        "test_df['label']  = test_df['label'].map(label_map)\n",
        "\n",
        "# Save filtered files\n",
        "train_df.to_csv(\"train_topN.csv\", index=False)\n",
        "val_df.to_csv(\"val_topN.csv\", index=False)\n",
        "test_df.to_csv(\"test_topN.csv\", index=False)\n",
        "\n",
        "print(\"✅ Saved: train_topN.csv, val_topN.csv, test_topN.csv\")\n"
      ],
      "metadata": {
        "id": "sKmnGRxZj7x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Load\n",
        "train_df = pd.read_csv(\"/content/train_topN.csv\")\n",
        "val_df   = pd.read_csv(\"/content/val_topN.csv\")\n",
        "test_df  = pd.read_csv(\"/content/test_topN.csv\")\n",
        "\n",
        "# Inputs and labels\n",
        "X_train = train_df.drop(columns=[\"label\"]).values\n",
        "y_train = train_df[\"label\"].values\n",
        "X_val   = val_df.drop(columns=[\"label\"]).values\n",
        "y_val   = val_df[\"label\"].values\n",
        "X_test  = test_df.drop(columns=[\"label\"]).values\n",
        "y_test  = test_df[\"label\"].values\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor   = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Dataloaders\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)\n",
        "\n",
        "# Number of classes (should match top_N)\n",
        "num_classes = len(np.unique(y_train))\n"
      ],
      "metadata": {
        "id": "JohZ1DSJj8Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weights = torch.softmax(self.attn(x), dim=1)\n",
        "        context = torch.sum(weights * x, dim=1)\n",
        "        return context\n",
        "\n",
        "class CNN_BiLSTM_Attention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bilstm = nn.LSTM(128, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.attn = Attention(hidden_dim * 2)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)            # (B, 1, F)\n",
        "        x = self.relu(self.conv1(x)) # (B, 128, F)\n",
        "        x = x.permute(0, 2, 1)       # (B, T, D)\n",
        "        x, _ = self.bilstm(x)\n",
        "        context = self.attn(x)\n",
        "        return self.fc(context)\n"
      ],
      "metadata": {
        "id": "0VsRXlcdlIx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Class weights for imbalanced classes\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "model = CNN_BiLSTM_Attention(input_dim=X_train.shape[1], hidden_dim=256, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n"
      ],
      "metadata": {
        "id": "HFUTyiyslNX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(1, 50):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(xb)\n",
        "        loss = criterion(outputs, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            val_loss += criterion(out, yb).item()\n",
        "            val_preds = torch.argmax(out, dim=1)\n",
        "            val_correct += (val_preds == yb).sum().item()\n",
        "            val_total += yb.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"⛔ Early stopping triggered.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "rHZ8aMialPKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Step 1: Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "classes = ['dos', 'ddos', 'injection', 'mitm', 'normal', 'password', 'scanning', 'xss', 'backdoor', 'ransomware']\n",
        "\n",
        "# Step 2: Normalize (for % values)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Step 3: Plot with seaborn heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.set(font_scale=1.2)  # adjust font size\n",
        "ax = sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes, linewidths=0.5, square=True)\n",
        "\n",
        "# Step 4: Labels and formatting\n",
        "plt.ylabel('True Label', fontsize=14)\n",
        "plt.xlabel('Predicted Label', fontsize=14)\n",
        "plt.title('Normalized Confusion Matrix', fontsize=16)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Step 5: Save for thesis use (e.g., PDF, PNG)\n",
        "plt.savefig(\"confusion_matrix_thesis.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dviRikB6-Mt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "sns.heatmap(df_report.iloc[:-1, :-1], annot=True, cmap=\"YlGnBu\")\n",
        "plt.title(\"Precision, Recall, F1-score per Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0AbpWug7-ugh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Get predictions for the test set\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    model_outputs = []\n",
        "    all_labels = []\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        outputs = model(xb)\n",
        "        model_outputs.append(outputs)  # Store model outputs\n",
        "        all_labels.append(yb)\n",
        "                          # Concatenate predictions and labels\n",
        "\n",
        "# FIX: Concatenate all_labels and model_outputs outside the loop\n",
        "model_outputs = torch.cat(model_outputs, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "y_test_bin = label_binarize(all_labels.cpu(), classes=range(len(classes)))\n",
        "y_score = model_outputs.detach().cpu().numpy()  # use softmax/logits output from model\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(len(classes)):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(len(classes)):\n",
        "    plt.plot(fpr[i], tpr[i], label=f\"{classes[i]} (AUC = {roc_auc[i]:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Multi-Class ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Gz3DNI7w-xwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize early stopping and tracking variables\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "wait = 0\n",
        "\n",
        "# Lists for plotting\n",
        "train_acc_list, val_acc_list = [], []\n",
        "train_loss_list, val_loss_list = [], []\n",
        "\n",
        "for epoch in range(1, 50):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(xb)\n",
        "        loss = criterion(outputs, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc_list.append(train_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_total, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            val_loss_total += loss.item() * yb.size(0)  # weight by batch size\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_correct += (preds == yb).sum().item()\n",
        "            val_total += yb.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_loss = val_loss_total / val_total\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "\n",
        "    # Logging\n",
        "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Step learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"⛔ Early stopping triggered.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "arc-slqOA5-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on test set\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        outputs = model(xb)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "# Compute Test Accuracy\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "test_acc = np.mean(all_preds == all_labels)\n",
        "print(f\"\\n✅ Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=classes))\n",
        "\n",
        "# Confusion Matrix (Normalized)\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Blues, xticks_rotation=45, colorbar=True)\n",
        "plt.title(\"Normalized Confusion Matrix - Test Set\")\n",
        "plt.tight_layout()\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0FA_OY7nBVVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# ─── Replace with your actual class names ───────────────────────────\n",
        "class_names = ['dos', 'ddos', 'injection', 'mitm', 'normal',\n",
        "               'password', 'scanning', 'xss', 'backdoor', 'ransomware']\n",
        "\n",
        "# Assuming 'all_labels' are your true labels and 'all_preds' are your predictions\n",
        "# ─── Generate confusion matrix ──────────────────────────────────────\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=range(len(class_names)))\n",
        "\n",
        "# ─── Normalize if needed ────────────────────────────────────────────\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# ─── Plot ───────────────────────────────────────────────────────────\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Proportion'}, linewidths=0.5, square=True)\n",
        "\n",
        "plt.title(\"Normalized Confusion Matrix\", fontsize=16)\n",
        "plt.xlabel(\"Predicted Label\", fontsize=14)\n",
        "plt.ylabel(\"True Label\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix_thesis.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wglXPX2bYPU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_acc_list, label='Train Accuracy', marker='o')\n",
        "plt.plot(val_acc_list, label='Validation Accuracy', marker='s')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Loss Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_loss_list, label='Train Loss', marker='o')\n",
        "plt.plot(val_loss_list, label='Validation Loss', marker='s')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QyR2VG0AA5gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get number of classes\n",
        "n_classes = len(classes)\n",
        "\n",
        "# Convert labels to one-hot\n",
        "y_true_bin = label_binarize(all_labels, classes=range(n_classes))\n",
        "\n",
        "# Get predicted probabilities\n",
        "model.eval()\n",
        "y_scores = []\n",
        "y_true_raw = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        outputs = model(xb)\n",
        "        probs = torch.softmax(outputs, dim=1)  # probabilities\n",
        "        y_scores.append(probs.cpu().numpy())\n",
        "        y_true_raw.append(yb.cpu().numpy())\n",
        "\n",
        "y_scores = np.vstack(y_scores)\n",
        "y_true_bin = label_binarize(np.concatenate(y_true_raw), classes=range(n_classes))\n",
        "\n",
        "# Compute ROC curve and AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], label=f\"{classes[i]} (AUC = {roc_auc[i]:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"Multi-Class ROC Curves (One-vs-Rest)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_La-fWxlBHPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sample a subset\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(labels), size=5000, replace=False)\n",
        "sampled_features = features[sample_indices]\n",
        "sampled_labels = labels[sample_indices]\n",
        "\n",
        "# Apply t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "features_2d = tsne.fit_transform(sampled_features)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=features_2d[:, 0],\n",
        "    y=features_2d[:, 1],\n",
        "    hue=sampled_labels,\n",
        "    palette='tab10',\n",
        "    s=60,\n",
        "    alpha=0.8,\n",
        "    legend='full'\n",
        ")\n",
        "plt.title(\"t-SNE of Test Set Feature Embeddings (Sampled)\", fontsize=15)\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"tsne_testset.png\", dpi=300)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xacNdcQVBe5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Sample a subset if needed (reuse from t-SNE step)\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(labels), size=5000, replace=False)\n",
        "sampled_features = features[sample_indices]\n",
        "sampled_labels = labels[sample_indices]\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(sampled_features)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=pca_result[:, 0],\n",
        "    y=pca_result[:, 1],\n",
        "    hue=sampled_labels,\n",
        "    palette='tab10',\n",
        "    s=60,\n",
        "    alpha=0.8,\n",
        "    legend='full'\n",
        ")\n",
        "plt.title(\"PCA of Test Set Feature Embeddings (Sampled)\", fontsize=15)\n",
        "plt.xlabel(f\"PC 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
        "plt.ylabel(f\"PC 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"pca_testset.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W2TDER5tBeuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BiLSTM Model Setup (Full Feature Set)"
      ],
      "metadata": {
        "id": "CnNdNTTbeYS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "cshgSY86f2Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, dropout=0.3):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "        self.bilstm = nn.LSTM(input_size=input_size,\n",
        "                              hidden_size=hidden_size,\n",
        "                              num_layers=num_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True,\n",
        "                              dropout=dropout if num_layers > 1 else 0.0)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add seq_len=1\n",
        "        out, _ = self.bilstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "x9i7iIGPf4Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual datasets\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=256, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=256, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EcwFKh96f6oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]\n",
        "model = BiLSTMClassifier(input_size=input_size, hidden_size=128, num_classes=10).to(device)\n",
        "\n",
        "# Class weights\n",
        "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "wait = 0\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    model.train()\n",
        "    total, correct, total_loss = 0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(1) == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            val_loss += criterion(out, yb).item()\n",
        "            val_correct += (out.argmax(1) == yb).sum().item()\n",
        "            val_total += yb.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_loss /= len(val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"best_bilstm.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"⏹️ Early stopping.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "c29rCHfYf9Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"best_bilstm.pt\"))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        probs = torch.softmax(out, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        y_true.extend(yb.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "# Classification report\n",
        "print(\"✅ Test Accuracy:\", np.mean(np.array(y_true) == np.array(y_pred)))\n",
        "print(\"\\n📋 Classification Report:\\n\", classification_report(y_true, y_pred, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "uWg9umPugAZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Proportion'}, linewidths=0.5)\n",
        "plt.title(\"BiLSTM Confusion Matrix (Normalized)\", fontsize=15)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bilstm_confusion_matrix.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YDZ-U0rAwOQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Binarize labels for One-vs-Rest AUC\n",
        "y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
        "y_prob = np.array(y_prob)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"BiLSTM ROC Curves — Multi-class (One-vs-Rest)\", fontsize=15)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bilstm_roc_curves.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1pJGsT6Iw2su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample 5000 points for t-SNE\n",
        "np.random.seed(42)\n",
        "idx = np.random.choice(len(y_true), 5000, replace=False)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "tsne_results = tsne.fit_transform(y_prob[idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1],\n",
        "                hue=np.array(y_true)[idx], palette='tab10', s=50, alpha=0.8, legend='full')\n",
        "plt.title(\"t-SNE of BiLSTM Test Set Embeddings\")\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bilstm_tsne.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r_8FLYKOw5Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_results = pca.fit_transform(y_prob[idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=pca_results[:, 0], y=pca_results[:, 1],\n",
        "                hue=np.array(y_true)[idx], palette='tab10', s=50, alpha=0.8, legend='full')\n",
        "plt.title(\"PCA of BiLSTM Test Set Embeddings\")\n",
        "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
        "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bilstm_pca.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4JTiU8qFw67O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full LSTM Model Pipeline (Using Normal Dataset)"
      ],
      "metadata": {
        "id": "HrXvTTtExCbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, dropout=0.3):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout if num_layers > 1 else 0.0)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Shape: (batch, seq_len=1, features)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # Use last time step\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "pqBUC0v6xFH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]  # Full feature count\n",
        "model = LSTMClassifier(input_size=input_size, hidden_size=128, num_classes=10).to(device)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n"
      ],
      "metadata": {
        "id": "b9QTi2uVxHMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "wait = 0\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    model.train()\n",
        "    total, correct, total_loss = 0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(1) == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            val_loss += criterion(out, yb).item()\n",
        "            val_correct += (out.argmax(1) == yb).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "    val_loss /= len(val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_lstm.pt\")\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"⏹️ Early stopping.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "Eh9Z7tsSxI6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_lstm.pt\"))\n"
      ],
      "metadata": {
        "id": "hbGdNFsXxUnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "metadata": {
        "id": "5NTxdXcmxYDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best LSTM model\n",
        "model.load_state_dict(torch.load(\"best_lstm.pt\"))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        probs = torch.softmax(out, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        y_true.extend(yb.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_prob = np.array(y_prob)\n"
      ],
      "metadata": {
        "id": "6vritsHrxa_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"✅ Test Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "L4DL50gHxdFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Proportion'})\n",
        "plt.title(\"LSTM Confusion Matrix (Normalized)\", fontsize=15)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"lstm_confusion_matrix.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3DseS-x0xevh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"LSTM ROC Curves — Multi-class (One-vs-Rest)\", fontsize=15)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"lstm_roc_curves.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k6QFq5FHxgWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 5000 points for visualization\n",
        "np.random.seed(42)\n",
        "sample_idx = np.random.choice(len(y_true), size=5000, replace=False)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "tsne_result = tsne.fit_transform(y_prob[sample_idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=tsne_result[:, 0], y=tsne_result[:, 1],\n",
        "                hue=y_true[sample_idx], palette='tab10', s=50, alpha=0.8)\n",
        "plt.title(\"t-SNE of LSTM Test Set Embeddings\")\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"lstm_tsne.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(y_prob[sample_idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1],\n",
        "                hue=y_true[sample_idx], palette='tab10', s=50, alpha=0.8)\n",
        "plt.title(\"PCA of LSTM Test Set Embeddings\")\n",
        "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
        "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"lstm_pca.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RGxHId4uxiSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN + BiLSTM Model"
      ],
      "metadata": {
        "id": "C1dKqCLqAdoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN_BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(CNN_BiLSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.bilstm = nn.LSTM(input_size=64, hidden_size=128,\n",
        "                              num_layers=1, batch_first=True,\n",
        "                              bidirectional=True)\n",
        "\n",
        "        self.fc = nn.Linear(128 * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
        "        x = self.conv1(x)   # (batch, 64, features)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)    # (batch, 64, features/2)\n",
        "        x = x.permute(0, 2, 1)  # (batch, seq_len, features) for LSTM\n",
        "        out, _ = self.bilstm(x)\n",
        "        out = out[:, -1, :]  # last time step\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "84P0Og-1A0BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]\n",
        "model = CNN_BiLSTM(input_size=input_size, num_classes=10).to(device)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n"
      ],
      "metadata": {
        "id": "Dz6dXej_A2E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "wait = 0\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(1) == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            val_loss += criterion(out, yb).item()\n",
        "            val_correct += (out.argmax(1) == yb).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "    val_loss /= len(val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"best_cnn_bilstm.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"⏹️ Early stopping.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "yyAhhNUkA47K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_cnn_bilstm.pt\"))\n",
        "model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "5pJmn4tuBBFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_cnn_bilstm.pt\"))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        probs = torch.softmax(out, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        y_true.extend(yb.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_prob = np.array(y_prob)\n"
      ],
      "metadata": {
        "id": "Bs4Z_LW9BE3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "print(f\"✅ Test Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "zlKNdmV8BKz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Proportion'})\n",
        "plt.title(\"CNN + BiLSTM Confusion Matrix (Normalized)\", fontsize=15)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cnn_bilstm_confusion_matrix.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "orHbqV7PBMJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"CNN + BiLSTM ROC Curves — Multi-class\", fontsize=15)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cnn_bilstm_roc_curves.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JvAo0_e7BOrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Sample 5000 points for visualization\n",
        "np.random.seed(42)\n",
        "sample_idx = np.random.choice(len(y_true), size=5000, replace=False)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "tsne_result = tsne.fit_transform(y_prob[sample_idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=tsne_result[:, 0], y=tsne_result[:, 1],\n",
        "                hue=y_true[sample_idx], palette='tab10', s=50, alpha=0.8)\n",
        "plt.title(\"t-SNE of CNN + BiLSTM Test Embeddings\")\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cnn_bilstm_tsne.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(y_prob[sample_idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1],\n",
        "                hue=y_true[sample_idx], palette='tab10', s=50, alpha=0.8)\n",
        "plt.title(\"PCA of CNN + BiLSTM Test Embeddings\")\n",
        "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
        "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cnn_bilstm_pca.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C9HrY_a9BQt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " MLP Model — Full Code (With Class Weights & Evaluation)"
      ],
      "metadata": {
        "id": "b_6BmearWBxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "9nsRfDsDWBQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]\n",
        "model = MLP(input_size=input_size, num_classes=10).to(device)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n"
      ],
      "metadata": {
        "id": "fy4Jhc9CWkcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "wait = 0\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(1) == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            val_loss += criterion(out, yb).item()\n",
        "            val_correct += (out.argmax(1) == yb).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "    val_loss /= len(val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_mlp.pt\")\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"⏹️ Early stopping.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "OJvdw_s2Wnbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_mlp.pt\"))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        probs = torch.softmax(out, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        y_true.extend(yb.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "U2hXOr-iWpG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "print(f\"✅ Test Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "zRvfj8ytWxG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Proportion'})\n",
        "plt.title(\"MLP Confusion Matrix (Normalized)\", fontsize=15)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mlp_confusion_matrix.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yMQ25pNUWy8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "y_prob = np.array(y_prob)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"MLP ROC Curves — Multi-class\", fontsize=15)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mlp_roc_curves.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jzC1W8pDW0l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Sample 5000 for visual clarity\n",
        "np.random.seed(42)\n",
        "sample_idx = np.random.choice(len(y_true), size=5000, replace=False)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "tsne_result = tsne.fit_transform(y_prob[sample_idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "# FIX: Convert y_true to a NumPy array for proper indexing\n",
        "sns.scatterplot(x=tsne_result[:, 0], y=tsne_result[:, 1],\n",
        "                hue=np.array(y_true)[sample_idx], palette='tab10', s=50, alpha=0.8)\n",
        "plt.title(\"t-SNE of MLP Test Embeddings\")\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mlp_tsne.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(y_prob[sample_idx])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "# FIX: Convert y_true to a NumPy array for proper indexing\n",
        "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1],\n",
        "                hue=np.array(y_true)[sample_idx], palette='tab10', s=50, alpha=0.8)\n",
        "plt.title(\"PCA of MLP Test Embeddings\")\n",
        "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
        "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
        "plt.legend(title=\"Class\", loc=\"best\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mlp_pca.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dB_CYJ5HW2zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Model names\n",
        "models = ['MLP', 'LSTM', 'BiLSTM', 'CNN+BiLSTM', 'CNN+BiLSTM+Attn']\n",
        "\n",
        "# Metrics\n",
        "accuracy = [0.61, 0.698, 0.702, 0.869, 0.887]\n",
        "macro_f1 = [0.55, 0.64, 0.64, 0.79, 0.82]\n",
        "\n",
        "x = np.arange(len(models))  # label locations\n",
        "width = 0.35  # width of bars\n"
      ],
      "metadata": {
        "id": "VApV8g4tdSJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars1 = ax.bar(x - width/2, accuracy, width, label='Accuracy')\n",
        "bars2 = ax.bar(x + width/2, macro_f1, width, label='Macro F1')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars1 + bars2:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.2f}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),  # vertical offset\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "# Axis formatting\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('DL Model Performance Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models, rotation=15)\n",
        "ax.set_ylim(0, 1.05)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot\n",
        "plt.savefig(\"dl_model_comparison.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DdzIJUPkdU3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Actual class names used in dataset\n",
        "class_names = [\n",
        "    'dos', 'ddos', 'injection', 'mitm', 'normal',\n",
        "    'password', 'scanning', 'xss', 'backdoor', 'ransomware'\n",
        "]\n",
        "\n",
        "# Simulated data placeholders (replace with real output from your models)\n",
        "models = ['MLP', 'LSTM', 'BiLSTM', 'CNN+BiLSTM', 'CNN+BiLSTM+Attn']\n",
        "accuracy = [0.61, 0.698, 0.702, 0.869, 0.887]\n",
        "macro_f1 = [0.55, 0.64, 0.64, 0.79, 0.82]\n",
        "conf_matrices = [np.random.rand(10, 10) for _ in models]\n",
        "roc_data = [np.linspace(0, 1, 100) for _ in models]\n",
        "tsne_data = [np.random.randn(500, 2) for _ in models]\n",
        "pca_data = [np.random.randn(500, 2) for _ in models]\n",
        "labels = np.random.randint(0, 10, 500)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Bar Plot: Accuracy vs Macro F1\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars1 = ax.bar(x - width/2, accuracy, width, label='Accuracy')\n",
        "bars2 = ax.bar(x + width/2, macro_f1, width, label='Macro F1')\n",
        "for bar in bars1 + bars2:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('DL Model Comparison: Accuracy vs Macro F1')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models, rotation=15)\n",
        "ax.set_ylim(0, 1.05)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Confusion Matrices (first 3 models)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, ax in enumerate(axes):\n",
        "    sns.heatmap(conf_matrices[i], ax=ax, cmap='Blues', annot=False,\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    ax.set_title(f\"{models[i]} Confusion Matrix\")\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# ROC Curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, model in enumerate(models):\n",
        "    fpr = roc_data[i]\n",
        "    tpr = roc_data[i] ** (1 + 0.1*i)  # simulate different shapes\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{model} (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"ROC Curve Comparison Across Models\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# t-SNE Embeddings\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, ax in enumerate(axes):\n",
        "    sns.scatterplot(x=tsne_data[i][:, 0], y=tsne_data[i][:, 1],\n",
        "                    hue=[class_names[l] for l in labels],\n",
        "                    palette='tab10', ax=ax, legend=False)\n",
        "    ax.set_title(f\"t-SNE: {models[i]}\")\n",
        "    ax.set_xlabel(\"Dim 1\")\n",
        "    ax.set_ylabel(\"Dim 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# PCA Embeddings\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, ax in enumerate(axes):\n",
        "    sns.scatterplot(x=pca_data[i][:, 0], y=pca_data[i][:, 1],\n",
        "                    hue=[class_names[l] for l in labels],\n",
        "                    palette='tab10', ax=ax, legend=False)\n",
        "    ax.set_title(f\"PCA: {models[i]}\")\n",
        "    ax.set_xlabel(\"PC1\")\n",
        "    ax.set_ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FYFPStYKde1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Radar chart setup\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'Macro F1']\n",
        "num_vars = len(labels)\n",
        "\n",
        "# Sample model metrics (replace with actual if needed)\n",
        "mlp_metrics =     [0.61, 0.60, 0.65, 0.55]\n",
        "lstm_metrics =    [0.698, 0.69, 0.72, 0.64]\n",
        "bilstm_metrics =  [0.702, 0.70, 0.73, 0.64]\n",
        "cnn_bilstm =      [0.869, 0.87, 0.88, 0.79]\n",
        "cnn_bilstm_attn = [0.887, 0.89, 0.90, 0.82]\n",
        "\n",
        "model_scores = [mlp_metrics, lstm_metrics, bilstm_metrics, cnn_bilstm, cnn_bilstm_attn]\n",
        "model_names = ['MLP', 'LSTM', 'BiLSTM', 'CNN+BiLSTM', 'CNN+BiLSTM+Attn']\n",
        "\n",
        "# Radar chart angles\n",
        "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "# Plot each model\n",
        "for scores, name in zip(model_scores, model_names):\n",
        "    data = scores + scores[:1]\n",
        "    ax.plot(angles, data, label=name)\n",
        "    ax.fill(angles, data, alpha=0.1)\n",
        "\n",
        "# Labels and legend\n",
        "ax.set_theta_offset(np.pi / 2)\n",
        "ax.set_theta_direction(-1)\n",
        "ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "ax.set_ylim(0, 1.1)\n",
        "plt.title(\"Radar Chart: DL Model Performance\", size=16)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zg6E4l9nemC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ml models"
      ],
      "metadata": {
        "id": "c1qstkXyci1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Train model\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_prob_rf = rf_model.predict_proba(X_test)\n",
        "\n",
        "# Basic results\n",
        "print(f\"✅ Test Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "ye3qTTcBcy6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_rf, normalize='true')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Random Forest — Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IALosz4tfQbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Binarize true labels\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob_rf[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"Random Forest — ROC Curve (Multiclass)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\", fontsize=9)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S0OENHKzfSQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' that you used to create X_train\n",
        "feature_names = df.drop(columns=['attack_type', 'label_enc']).columns  # Get feature names from original DataFrame\n",
        "\n",
        "# Train model\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_prob_rf = rf_model.predict_proba(X_test)\n",
        "\n",
        "# Basic results\n",
        "print(f\"✅ Test Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=class_names))\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)[-15:]  # top 15\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "\n",
        "# Use feature_names instead of X_train.columns\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"Random Forest — Top 15 Important Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "459GsIsQfT-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Train model\n",
        "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10,\n",
        "                          objective='multi:softprob', num_class=10,\n",
        "                          eval_metric='mlogloss', use_label_encoder=False,\n",
        "                          n_jobs=-1, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_prob_xgb = xgb_model.predict_proba(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f\"✅ Test Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "v-UBM6tTjvs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_xgb, normalize='true')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"XGBoost — Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ugrj_EfjjyT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob_xgb[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"XGBoost — ROC Curve (Multiclass)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\", fontsize=9)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d3Ga3inVkEBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb  # Import the xgboost module\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Call plot_importance from xgboost, passing the booster\n",
        "xgb.plot_importance(xgb_model.get_booster(), importance_type='gain', max_num_features=15)\n",
        "plt.title(\"XGBoost — Top 15 Important Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8m6HV9cJkFzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Train model\n",
        "lgb_model = lgb.LGBMClassifier(objective='multiclass',\n",
        "                               num_class=10,\n",
        "                               n_estimators=200,\n",
        "                               learning_rate=0.1,\n",
        "                               max_depth=10,\n",
        "                               class_weight='balanced',\n",
        "                               random_state=42,\n",
        "                               n_jobs=-1)\n",
        "\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "y_prob_lgb = lgb_model.predict_proba(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f\"✅ Test Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}\")\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_lgb, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "f74lws4MkIOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_lgb, normalize='true')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"LightGBM — Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VS8xLbs1kL8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob_lgb[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"LightGBM — ROC Curve (Multiclass)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\", fontsize=9)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9eh_yRo0kNwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "lgb.plot_importance(lgb_model, max_num_features=15, importance_type='gain')\n",
        "plt.title(\"LightGBM — Top 15 Important Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zru3pXHIkPsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plots for thesis"
      ],
      "metadata": {
        "id": "Hsg2hAH29TDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown silently\n",
        "!pip install --quiet gdown\n",
        "\n",
        "# Import libraries\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# Updated Google Drive shareable link\n",
        "url = \"https://drive.google.com/uc?id=1t0mqjuEHOu_WZWh6wI_As1KvdZWHixHv\"\n",
        "output = \"new_dataset.csv\"  # You can rename this as needed\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Load the CSV into a DataFrame\n",
        "df = pd.read_csv(output)\n",
        "print(\"✅ Shape of the dataset:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "ZvwwVrTt9VgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M_B_Q0S49_41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load uploaded dataset\n",
        "df = pd.read_csv(\"/content/new_dataset.csv\")\n",
        "\n",
        "# Step 1: Clean attack_type labels\n",
        "df['attack_type'] = df['attack_type'].str.lower().replace({\n",
        "    'udp-lag': 'udplag', 'udp_lag': 'udplag',\n",
        "    'drdos_mssql': 'drdos_mssql', 'drdos_msssql': 'drdos_mssql',\n",
        "    'drdos_dns': 'drdos_dns', 'drdos_snmp': 'drdos_snmp',\n",
        "    'drdos_ldap': 'drdos_ldap', 'drdos_netbios': 'drdos_netbios',\n",
        "    'normal': 'benign', 'webddos': 'webddos'\n",
        "})\n",
        "\n",
        "# Count attacks\n",
        "attack_counts = df['attack_type'].value_counts()\n",
        "\n",
        "# Plot 1: Log-scaled full attack distribution\n",
        "plt.figure(figsize=(18, 8))\n",
        "bars = plt.bar(attack_counts.index, attack_counts.values, color='steelblue')\n",
        "plt.yscale('log')\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    if height > 0:\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, height * 1.1,\n",
        "                 f'{int(height):,}', ha='center', va='bottom', fontsize=9, rotation=90)\n",
        "\n",
        "plt.title('Log-Scaled Distribution of Cleaned Attack Types', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Attack Type', fontsize=14)\n",
        "plt.ylabel('Number of Samples (Log Scale)', fontsize=14)\n",
        "plt.xticks(rotation=75, ha='right', fontsize=11)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "7aRF833Y_ACE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load raw combined dataset\n",
        "df_raw = pd.read_csv(\"/content/new_dataset.csv\")  # Replace with your actual path\n",
        "print(\"✅ Raw dataset shape:\", df_raw.shape)\n",
        "df_raw.head()\n"
      ],
      "metadata": {
        "id": "bcQ3droOBUWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🔍 Raw Dataset Info:\")\n",
        "print(df_raw.info())\n",
        "print(\"\\n🔍 Missing values (before cleaning):\")\n",
        "print(df_raw.isnull().sum())\n",
        "print(\"\\n🔍 Class Distribution (before):\")\n",
        "print(df_raw['attack_type'].value_counts())\n"
      ],
      "metadata": {
        "id": "Q4EzLSuRBzjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📋 Current Columns:\", df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "eLoeZYHQEl68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Reload raw dataset\n",
        "df_raw = pd.read_csv(\"/content/new_dataset.csv\", low_memory=False)\n",
        "df_raw.columns = df_raw.columns.str.strip().str.lower()\n",
        "\n",
        "# Step 2: Clean attack_type column\n",
        "df = df_raw.copy()\n",
        "df['attack_type'] = df['attack_type'].astype(str).str.strip().str.upper()\n",
        "df['attack_type'] = df['attack_type'].replace({'NAN': np.nan, '1.0': 'BENIGN', '0.0': 'BENIGN', 'LABEL': np.nan})\n",
        "\n",
        "# Step 3: Drop only rows without attack_type\n",
        "df = df.dropna(subset=['attack_type'])\n",
        "\n",
        "# Step 4: Convert object → numeric\n",
        "numeric_columns = ['flow_duration', 'dst_port', 'total_fwd_pkts', 'total_bwd_pkts', 'flow_bytes_s', 'flow_pkts_s']\n",
        "for col in numeric_columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Step 5: Impute missing numeric values with median\n",
        "for col in numeric_columns:\n",
        "    median_val = df[col].median()\n",
        "    df[col].fillna(median_val, inplace=True)\n",
        "\n",
        "# Step 6: Encode protocol (categorical)\n",
        "df['protocol'], _ = pd.factorize(df['protocol'])\n",
        "\n",
        "# Step 7: Encode attack_type\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_target = LabelEncoder()\n",
        "df['attack_type_enc'] = le_target.fit_transform(df['attack_type'])\n",
        "\n",
        "# ✅ Diagnostic: Print shape\n",
        "print(\"✅ Cleaned dataset shape:\", df.shape)\n",
        "\n",
        "# Step 7.5: Replace inf/-inf with NaN, then impute with median\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "\n",
        "# Step 8: Plot class distribution before SMOTE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(y='attack_type', data=df, order=df['attack_type'].value_counts().index)\n",
        "plt.title(\"📊 Class Distribution Before Preprocessing\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "features = df.drop(columns=['attack_type', 'attack_type_enc'])\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Step 10: Apply SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_scaled, df['attack_type_enc'])\n",
        "\n",
        "# Step 11: After SMOTE plot\n",
        "df_after = pd.DataFrame(X_resampled, columns=features.columns)\n",
        "df_after['attack_type_enc'] = y_resampled\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='attack_type_enc', data=df_after, order=sorted(df_after['attack_type_enc'].unique()))\n",
        "plt.title(\"✅ Class Distribution After SMOTE + Encoding\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 12: Label Mapping\n",
        "label_mapping = dict(zip(le_target.transform(le_target.classes_), le_target.classes_))\n",
        "print(\"📌 Encoded Label Mapping:\")\n",
        "for k, v in label_mapping.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "AiTws8EtEtEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}